"""
SAINTä¼˜åŒ–è®­ç»ƒè„šæœ¬
é›†æˆæ€§èƒ½ä¼˜åŒ–ã€æ··åˆç²¾åº¦è®­ç»ƒã€é…ç½®ç®¡ç†ç­‰åŠŸèƒ½
"""
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torch.optim as optim
from torch.cuda.amp import GradScaler, autocast
import os
import time
import numpy as np
from tqdm import tqdm

# å¯¼å…¥é¡¹ç›®æ¨¡å—
try:
    from config import MODEL_CONFIG, TRAINING_CONFIG, PERFORMANCE_CONFIG, auto_adjust_config
    from saint.models import SAINT
    from saint.data_utils import DataSetCatCon
    from saint.augmentations import embed_data_mask
    from saint.utils import classification_scores
    USE_CONFIG = True
except ImportError as e:
    print(f"âš ï¸ é…ç½®å¯¼å…¥å¤±è´¥: {e}")
    print("ğŸ“Œ ä½¿ç”¨é»˜è®¤é…ç½®")
    USE_CONFIG = False

def load_processed_data(data_dir="processed_data"):
    """åŠ è½½é¢„å¤„ç†åçš„æ•°æ®"""
    print("ğŸ“‚ åŠ è½½é¢„å¤„ç†æ•°æ®...")
    
    import pandas as pd
    import joblib
    
    # åŠ è½½ç‰¹å¾æ•°æ®
    X_train = pd.read_csv(os.path.join(data_dir, 'X_train_processed.csv'))
    X_val = pd.read_csv(os.path.join(data_dir, 'X_val_processed.csv'))
    X_test = pd.read_csv(os.path.join(data_dir, 'X_test_processed.csv'))
    
    # åŠ è½½æ ‡ç­¾æ•°æ®
    y_train = pd.read_csv(os.path.join(data_dir, 'y_train_processed.csv')).values.flatten()
    y_val = pd.read_csv(os.path.join(data_dir, 'y_val_processed.csv')).values.flatten()
    y_test = pd.read_csv(os.path.join(data_dir, 'y_test_processed.csv')).values.flatten()
    
    # åŠ è½½é¢„å¤„ç†å™¨
    label_encoders = joblib.load(os.path.join(data_dir, 'label_encoders.joblib'))
    
    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: Train {X_train.shape}, Val {X_val.shape}, Test {X_test.shape}")
    
    return X_train, X_val, X_test, y_train, y_val, y_test, label_encoders

def create_data_loaders(X_train, X_val, X_test, y_train, y_val, y_test, label_encoders):
    """åˆ›å»ºä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨"""
    print("ğŸ”„ åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    
    # ç¡®å®šç±»åˆ«ç‰¹å¾å’Œè¿ç»­ç‰¹å¾çš„ç´¢å¼•
    cat_cols = list(label_encoders.keys())
    all_cols = X_train.columns.tolist()
    cat_idxs = [all_cols.index(col) for col in cat_cols if col in all_cols]
    con_idxs = [i for i in range(len(all_cols)) if i not in cat_idxs]
    
    # è½¬æ¢ä¸ºæ•°æ®é›†æ ¼å¼
    def prepare_data(X, y):
        return {
            'data': X.values,
            'mask': np.ones_like(X.values)  # å‡è®¾æ— ç¼ºå¤±å€¼
        }, {'data': y.reshape(-1, 1)}
    
    X_train_dict, y_train_dict = prepare_data(X_train, y_train)
    X_val_dict, y_val_dict = prepare_data(X_val, y_val)
    X_test_dict, y_test_dict = prepare_data(X_test, y_test)
    
    # è·å–é…ç½®
    if USE_CONFIG:
        batch_size = TRAINING_CONFIG['batch_size']
        num_workers = PERFORMANCE_CONFIG['num_workers']
        pin_memory = PERFORMANCE_CONFIG['pin_memory']
    else:
        batch_size, num_workers, pin_memory = 256, 4, True
    
    # åˆ›å»ºæ•°æ®é›†
    train_ds = DataSetCatCon(X_train_dict, y_train_dict, cat_idxs, 'clf')
    val_ds = DataSetCatCon(X_val_dict, y_val_dict, cat_idxs, 'clf')
    test_ds = DataSetCatCon(X_test_dict, y_test_dict, cat_idxs, 'clf')
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ (ä¼˜åŒ–é…ç½®)
    train_loader = DataLoader(
        train_ds, batch_size=batch_size, shuffle=True,
        num_workers=num_workers, pin_memory=pin_memory,
        persistent_workers=True if num_workers > 0 else False
    )
    val_loader = DataLoader(
        val_ds, batch_size=batch_size, shuffle=False,
        num_workers=num_workers, pin_memory=pin_memory
    )
    test_loader = DataLoader(
        test_ds, batch_size=batch_size, shuffle=False,
        num_workers=num_workers, pin_memory=pin_memory
    )
    
    print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ")
    return train_loader, val_loader, test_loader, cat_idxs, con_idxs

def create_model(X_train, y_train, label_encoders):
    """åˆ›å»ºå¹¶é…ç½®SAINTæ¨¡å‹"""
    print("ğŸ¤– åˆ›å»ºSAINTæ¨¡å‹...")
    
    # è®¡ç®—ç±»åˆ«ç»´åº¦
    cat_dims = [len(enc.classes_) for enc in label_encoders.values()]
    cat_dims = [1] + cat_dims  # æ·»åŠ [CLS] token
    
    num_continuous = len([col for col in X_train.columns if col not in label_encoders.keys()])
    num_classes = len(np.unique(y_train))
    
    # è‡ªåŠ¨è°ƒæ•´é…ç½®
    if USE_CONFIG:
        config = auto_adjust_config(X_train.shape[1], X_train.shape[0])
    else:
        config = {
            'embedding_size': 32, 'transformer_depth': 6, 'attention_heads': 8,
            'attention_dropout': 0.1, 'ff_dropout': 0.1,
            'categorical_embedding_type': 'saint', 'numerical_embedding_type': 'mlp',
            'attentiontype': 'colrow', 'final_mlp_style': 'common'
        }
    
    # åˆ›å»ºæ¨¡å‹
    model = SAINT(
        categories=tuple(cat_dims),
        num_continuous=num_continuous,
        dim=config['embedding_size'],
        depth=config['transformer_depth'],
        heads=config['attention_heads'],
        attn_dropout=config['attention_dropout'],
        ff_dropout=config['ff_dropout'],
        categorical_embedding_type=config['categorical_embedding_type'],
        numerical_embedding_type=config['numerical_embedding_type'],
        attentiontype=config['attentiontype'],
        final_mlp_style=config['final_mlp_style'],
        y_dim=num_classes
    )
    
    print(f"âœ… æ¨¡å‹åˆ›å»ºå®Œæˆ: {sum(p.numel() for p in model.parameters())/1e6:.2f}M å‚æ•°")
    return model, config

def train_model(model, train_loader, val_loader, config):
    """ä¼˜åŒ–çš„è®­ç»ƒå¾ªç¯"""
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    
    # è®¾å¤‡é…ç½®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {device}")
    
    # è·å–è®­ç»ƒé…ç½®
    if USE_CONFIG:
        epochs = TRAINING_CONFIG['epochs']
        lr = TRAINING_CONFIG['lr']
        optimizer_name = TRAINING_CONFIG['optimizer']
        use_amp = PERFORMANCE_CONFIG['mixed_precision']
        max_grad_norm = PERFORMANCE_CONFIG['max_grad_norm']
    else:
        epochs, lr, optimizer_name, use_amp, max_grad_norm = 100, 0.0001, 'AdamW', True, 1.0
    
    # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    if optimizer_name == 'AdamW':
        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)
    elif optimizer_name == 'Adam':
        optimizer = optim.Adam(model.parameters(), lr=lr)
    else:
        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)
    
    criterion = nn.CrossEntropyLoss()
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)
    
    # æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = GradScaler() if use_amp else None
    
    # è®­ç»ƒè®°å½•
    best_val_acc = 0.0
    train_losses, val_accs = [], []
    
    for epoch in range(epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        epoch_loss = 0.0
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}")
        
        for batch_idx, data in enumerate(progress_bar):
            x_categ, x_cont, y_gts, cat_mask, con_mask = [d.to(device) for d in data]
            
            optimizer.zero_grad()
            
            # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
            if use_amp:
                with autocast():
                    _, x_categ_enc, x_cont_enc = embed_data_mask(x_categ, x_cont, cat_mask, con_mask, model)
                    reps = model.transformer(x_categ_enc, x_cont_enc)
                    y_reps = reps[:, 0, :]
                    y_outs = model.mlpfory(y_reps)
                    loss = criterion(y_outs, y_gts.squeeze())
                
                # åå‘ä¼ æ’­
                scaler.scale(loss).backward()
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
                scaler.step(optimizer)
                scaler.update()
            else:
                _, x_categ_enc, x_cont_enc = embed_data_mask(x_categ, x_cont, cat_mask, con_mask, model)
                reps = model.transformer(x_categ_enc, x_cont_enc)
                y_reps = reps[:, 0, :]
                y_outs = model.mlpfory(y_reps)
                loss = criterion(y_outs, y_gts.squeeze())
                
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
                optimizer.step()
            
            epoch_loss += loss.item()
            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})
        
        scheduler.step()
        avg_loss = epoch_loss / len(train_loader)
        train_losses.append(avg_loss)
        
        # éªŒè¯é˜¶æ®µ
        if (epoch + 1) % 5 == 0:  # æ¯5ä¸ªepochéªŒè¯ä¸€æ¬¡
            val_acc, _ = classification_scores(model, val_loader, device, 'multiclass', False)
            val_accs.append(val_acc)
            
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                torch.save(model.state_dict(), 'best_saint_model.pth')
                print(f"ğŸ¯ æ–°çš„æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {val_acc:.4f}")
            
            print(f"Epoch {epoch+1}: Loss={avg_loss:.4f}, Val_Acc={val_acc:.4f}, Best={best_val_acc:.4f}")
    
    print(f"âœ… è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}")
    return model, train_losses, val_accs

def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    print("=" * 50)
    print("ğŸ¤– SAINT æ™ºèƒ½å®¶å±…è®¾å¤‡åˆ†ç±»è®­ç»ƒ")
    print("=" * 50)
    
    start_time = time.time()
    
    try:
        # 1. åŠ è½½æ•°æ®
        X_train, X_val, X_test, y_train, y_val, y_test, label_encoders = load_processed_data()
        
        # 2. åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader, val_loader, test_loader, cat_idxs, con_idxs = create_data_loaders(
            X_train, X_val, X_test, y_train, y_val, y_test, label_encoders
        )
        
        # 3. åˆ›å»ºæ¨¡å‹
        model, config = create_model(X_train, y_train, label_encoders)
        
        # 4. è®­ç»ƒæ¨¡å‹
        trained_model, train_losses, val_accs = train_model(model, train_loader, val_loader, config)
        
        # 5. æœ€ç»ˆæµ‹è¯•
        print("ğŸ§ª æœ€ç»ˆæµ‹è¯•è¯„ä¼°...")
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        test_acc, test_auc = classification_scores(trained_model, test_loader, device, 'multiclass', False)
        
        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ!")
        print(f"   æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}")
        print(f"   è®­ç»ƒç”¨æ—¶: {(time.time() - start_time)/60:.2f} åˆ†é’Ÿ")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 