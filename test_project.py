#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é¡¹ç›®å®Œæ•´æ€§æµ‹è¯•è„šæœ¬
æµ‹è¯•æ•´ä¸ª SAINT æ™ºèƒ½å®¶å±…è®¾å¤‡åˆ†ç±»é¡¹ç›®çš„å„ä¸ªç»„ä»¶
"""

import os
import sys
import torch
import numpy as np
import pandas as pd
from torch.utils.data import DataLoader
import traceback
from config import Config

def print_separator(title):
    """æ‰“å°åˆ†éš”ç¬¦"""
    print("\n" + "="*60)
    print(f" {title} ")
    print("="*60)

def test_imports():
    """æµ‹è¯•æ‰€æœ‰å¿…è¦çš„å¯¼å…¥"""
    print_separator("1. æµ‹è¯•å¯¼å…¥")
    
    try:
        from saint.data_utils import data_prep_custom, DataSetCatCon
        print("âœ“ saint.data_utils å¯¼å…¥æˆåŠŸ")
        
        from saint.models import SAINT
        print("âœ“ saint.models å¯¼å…¥æˆåŠŸ")
        
        from saint.augmentations import embed_data_mask
        print("âœ“ saint.augmentations å¯¼å…¥æˆåŠŸ")
        
        import config
        print("âœ“ config å¯¼å…¥æˆåŠŸ")
        
        return True
    except Exception as e:
        print(f"âœ— å¯¼å…¥å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_data_files():
    """æµ‹è¯•æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
    print_separator("2. æµ‹è¯•æ•°æ®æ–‡ä»¶")
    
    config = Config()
    
    # æ£€æŸ¥å¤„ç†åçš„æ•°æ®æ–‡ä»¶
    data_files = {
        'è®­ç»ƒæ•°æ®': config.TRAIN_DATA_PATH,
        'éªŒè¯æ•°æ®': config.VAL_DATA_PATH,
        'æµ‹è¯•æ•°æ®': config.TEST_DATA_PATH,
        'ç‰¹å¾ä¿¡æ¯': config.FEATURE_INFO_PATH
    }
    
    all_exist = True
    for name, path in data_files.items():
        if os.path.exists(path):
            size = os.path.getsize(path) / (1024*1024)  # MB
            print(f"âœ“ {name}: {path} (å¤§å°: {size:.1f}MB)")
        else:
            print(f"âœ— {name}: {path} ä¸å­˜åœ¨")
            all_exist = False
    
    return all_exist

def test_data_loading():
    """æµ‹è¯•æ•°æ®åŠ è½½"""
    print_separator("3. æµ‹è¯•æ•°æ®åŠ è½½")
    
    try:
        from saint.data_utils import data_prep_custom, DataSetCatCon
        
        config = Config()
        
        # åŠ è½½ç‰¹å¾ä¿¡æ¯
        print("åŠ è½½ç‰¹å¾ä¿¡æ¯...")
        feature_info = pd.read_csv(config.FEATURE_INFO_PATH)
        print(f"ç‰¹å¾ä¿¡æ¯å½¢çŠ¶: {feature_info.shape}")
        print(f"åˆ†ç±»ç‰¹å¾: {feature_info['feature_name'][feature_info['is_categorical']].tolist()}")
        print(f"æ•°å€¼ç‰¹å¾: {feature_info['feature_name'][~feature_info['is_categorical']].tolist()}")
        
        # åŠ è½½å°é‡æ•°æ®è¿›è¡Œæµ‹è¯•
        print("\nåŠ è½½è®­ç»ƒæ•°æ®æ ·æœ¬...")
        train_data = pd.read_csv(config.TRAIN_DATA_PATH, nrows=1000)
        print(f"è®­ç»ƒæ•°æ®å½¢çŠ¶: {train_data.shape}")
        print(f"è®¾å¤‡ç±»åˆ«åˆ†å¸ƒ:\n{train_data['device_category'].value_counts().head()}")
        
        return True
    except Exception as e:
        print(f"âœ— æ•°æ®åŠ è½½å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_model_initialization():
    """æµ‹è¯•æ¨¡å‹åˆå§‹åŒ–"""
    print_separator("4. æµ‹è¯•æ¨¡å‹åˆå§‹åŒ–")
    
    try:
        from saint.models import SAINT
        from saint.data_utils import data_prep_custom, DataSetCatCon
        
        config = Config()
        
        # åŠ è½½å°é‡æ•°æ®è¿›è¡Œæµ‹è¯•
        print("å‡†å¤‡æµ‹è¯•æ•°æ®...")
        feature_info = pd.read_csv(config.FEATURE_INFO_PATH)
        train_data = pd.read_csv(config.TRAIN_DATA_PATH, nrows=1000)
        
        cat_cols = feature_info['feature_name'][feature_info['is_categorical']].tolist()
        con_cols = feature_info['feature_name'][~feature_info['is_categorical']].tolist()
        
        X = train_data[cat_cols + con_cols]
        y = train_data['device_category']
        
        print(f"ç‰¹å¾æ•°é‡: åˆ†ç±»={len(cat_cols)}, æ•°å€¼={len(con_cols)}")
        
        # æ•°æ®é¢„å¤„ç†
        print("æ•°æ®é¢„å¤„ç†...")
        cat_dims, cat_idxs, con_idxs, X_train, y_train, X_valid, y_valid, X_test, y_test, train_mean, train_std, bins = data_prep_custom(
            X=X, y=y, cat_cols=cat_cols, con_cols=con_cols, task='multiclass'
        )
        
        print(f"ç±»åˆ«ç»´åº¦: {cat_dims}")
        print(f"ç±»åˆ«æ•°é‡: {len(set(y))}")
        
        # åˆå§‹åŒ–æ¨¡å‹
        print("åˆå§‹åŒ–æ¨¡å‹...")
        model = SAINT(
            categories=tuple(cat_dims),
            num_continuous=len(con_cols),
            dim=config.model_params['dim'],
            depth=config.model_params['depth'],
            heads=config.model_params['heads'],
            categorical_embedding_type='ft',
            numerical_embedding_type='ple',
            bins=bins,
            attentiontype=config.model_params['attentiontype'],
            y_dim=len(set(y))
        )
        
        # è®¡ç®—æ¨¡å‹å‚æ•°é‡
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        print(f"âœ“ æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        print(f"æ€»å‚æ•°é‡: {total_params:,}")
        print(f"å¯è®­ç»ƒå‚æ•°é‡: {trainable_params:,}")
        
        return True, (model, cat_dims, cat_idxs, con_idxs, X_train, y_train, X_valid, y_valid, train_mean, train_std, bins)
    except Exception as e:
        print(f"âœ— æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        traceback.print_exc()
        return False, None

def test_training_loop(model_data):
    """æµ‹è¯•è®­ç»ƒå¾ªç¯"""
    print_separator("5. æµ‹è¯•è®­ç»ƒå¾ªç¯")
    
    try:
        from saint.augmentations import embed_data_mask
        import torch.optim as optim
        from torch import nn
        
        model, cat_dims, cat_idxs, con_idxs, X_train, y_train, X_valid, y_valid, train_mean, train_std, bins = model_data
        config = Config()
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        print("åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
        continuous_mean_std = np.array([train_mean, train_std]).astype(np.float32)
        
        train_ds = DataSetCatCon(X_train, y_train, cat_idxs, task='clf', continuous_mean_std=continuous_mean_std)
        trainloader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=0)
        
        valid_ds = DataSetCatCon(X_valid, y_valid, cat_idxs, task='clf', continuous_mean_std=continuous_mean_std)
        validloader = DataLoader(valid_ds, batch_size=64, shuffle=False, num_workers=0)
        
        print(f"è®­ç»ƒæ‰¹æ¬¡æ•°: {len(trainloader)}")
        print(f"éªŒè¯æ‰¹æ¬¡æ•°: {len(validloader)}")
        
        # è®¾ç½®è®¾å¤‡å’Œä¼˜åŒ–å™¨
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")
        
        model.to(device)
        optimizer = optim.AdamW(model.parameters(), lr=config.training_params['lr'])
        criterion = nn.CrossEntropyLoss()
        
        # å¿«é€Ÿè®­ç»ƒæµ‹è¯•ï¼ˆåªè®­ç»ƒ2ä¸ªepochï¼‰
        print("\nå¼€å§‹å¿«é€Ÿè®­ç»ƒæµ‹è¯•...")
        for epoch in range(2):
            model.train()
            epoch_loss = 0.0
            num_batches = 0
            
            for i, data in enumerate(trainloader):
                if i >= 5:  # åªæµ‹è¯•å‰5ä¸ªæ‰¹æ¬¡
                    break
                    
                optimizer.zero_grad()
                x_categ, x_cont, y_gts, cat_mask, con_mask = [d.to(device) for d in data]
                
                # å‰å‘ä¼ æ’­
                _, x_categ_enc, x_cont_enc = embed_data_mask(x_categ, x_cont, cat_mask, con_mask, model)
                reps = model.transformer(x_categ_enc, x_cont_enc)
                y_reps = reps[:, 0, :]
                y_outs = model.mlpfory(y_reps)
                
                loss = criterion(y_outs, y_gts.squeeze())
                loss.backward()
                optimizer.step()
                
                epoch_loss += loss.item()
                num_batches += 1
            
            avg_loss = epoch_loss / num_batches if num_batches > 0 else 0
            print(f"Epoch {epoch+1}: å¹³å‡æŸå¤± = {avg_loss:.4f}")
        
        # éªŒè¯æµ‹è¯•
        print("\néªŒè¯æµ‹è¯•...")
        model.eval()
        correct = 0
        total = 0
        
        with torch.no_grad():
            for i, data in enumerate(validloader):
                if i >= 3:  # åªæµ‹è¯•å‰3ä¸ªæ‰¹æ¬¡
                    break
                    
                x_categ, x_cont, y_gts, cat_mask, con_mask = [d.to(device) for d in data]
                
                _, x_categ_enc, x_cont_enc = embed_data_mask(x_categ, x_cont, cat_mask, con_mask, model)
                reps = model.transformer(x_categ_enc, x_cont_enc)
                y_reps = reps[:, 0, :]
                y_outs = model.mlpfory(y_reps)
                
                _, predicted = torch.max(y_outs.data, 1)
                total += y_gts.size(0)
                correct += (predicted == y_gts.squeeze()).sum().item()
        
        accuracy = 100 * correct / total if total > 0 else 0
        print(f"æµ‹è¯•å‡†ç¡®ç‡: {accuracy:.2f}%")
        
        print("âœ“ è®­ç»ƒå¾ªç¯æµ‹è¯•æˆåŠŸ")
        return True
    except Exception as e:
        print(f"âœ— è®­ç»ƒå¾ªç¯æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_optimized_training():
    """æµ‹è¯•ä¼˜åŒ–çš„è®­ç»ƒè„šæœ¬"""
    print_separator("6. æµ‹è¯•ä¼˜åŒ–è®­ç»ƒè„šæœ¬")
    
    try:
        import train_optimized
        print("âœ“ train_optimized.py å¯ä»¥å¯¼å…¥")
        
        # æ£€æŸ¥å…³é”®å‡½æ•°æ˜¯å¦å­˜åœ¨
        functions_to_check = ['create_model', 'create_data_loaders', 'train_epoch', 'validate']
        for func_name in functions_to_check:
            if hasattr(train_optimized, func_name):
                print(f"âœ“ {func_name} å‡½æ•°å­˜åœ¨")
            else:
                print(f"âœ— {func_name} å‡½æ•°ç¼ºå¤±")
        
        return True
    except Exception as e:
        print(f"âœ— ä¼˜åŒ–è®­ç»ƒè„šæœ¬æµ‹è¯•å¤±è´¥: {e}")
        return False

def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("SAINT æ™ºèƒ½å®¶å±…è®¾å¤‡åˆ†ç±»é¡¹ç›® - å®Œæ•´æ€§æµ‹è¯•")
    print("="*60)
    
    all_passed = True
    
    # 1. æµ‹è¯•å¯¼å…¥
    if not test_imports():
        all_passed = False
        return False
    
    # 2. æµ‹è¯•æ•°æ®æ–‡ä»¶
    if not test_data_files():
        all_passed = False
        print("\nâš ï¸  æ•°æ®æ–‡ä»¶ç¼ºå¤±ï¼Œè¯·å…ˆè¿è¡Œ data-processing.py ç”Ÿæˆæ•°æ®")
        return False
    
    # 3. æµ‹è¯•æ•°æ®åŠ è½½
    if not test_data_loading():
        all_passed = False
        return False
    
    # 4. æµ‹è¯•æ¨¡å‹åˆå§‹åŒ–
    model_success, model_data = test_model_initialization()
    if not model_success:
        all_passed = False
        return False
    
    # 5. æµ‹è¯•è®­ç»ƒå¾ªç¯
    if not test_training_loop(model_data):
        all_passed = False
        return False
    
    # 6. æµ‹è¯•ä¼˜åŒ–è®­ç»ƒè„šæœ¬
    if not test_optimized_training():
        all_passed = False
    
    # æ€»ç»“
    print_separator("æµ‹è¯•æ€»ç»“")
    if all_passed:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼é¡¹ç›®å‡†å¤‡å°±ç»ª")
        print("\nä¸‹ä¸€æ­¥å»ºè®®:")
        print("1. è¿è¡Œå®Œæ•´è®­ç»ƒ: python train_optimized.py")
        print("2. æˆ–ä½¿ç”¨å¼€å‘æ¨¡å¼: python train_optimized.py --dev")
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯ä¿¡æ¯")
    
    return all_passed

if __name__ == "__main__":
    success = run_all_tests()
    sys.exit(0 if success else 1) 